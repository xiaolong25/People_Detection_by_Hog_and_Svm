{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load data module\n",
    "The dataset is divided into training and testing sets, each containing positive and negative samples\n",
    "\n",
    "Dataset\n",
    "│\n",
    "├── train                # Training dataset\n",
    "│   ├── pos              # Positive samples\n",
    "│   └── neg              # Negative samples\n",
    "│\n",
    "└── test                 # Testing dataset\n",
    "    ├── pos              # Positive samples\n",
    "    └── neg              # Negative samples\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Traverse the directory to load images and resize them to the specified size\n",
    "def traversal_data(path, image_size):\n",
    "    images_list = []\n",
    "    for file in os.listdir(path):\n",
    "        img_path = os.path.join(path, file)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, image_size)\n",
    "            images_list.append(img)\n",
    "    return images_list\n",
    "\n",
    "# Load positive and negative samples from the given directories\n",
    "def load_dataset(pos_dir, neg_dir, image_size):\n",
    "    positive =  traversal_data(pos_dir, image_size)\n",
    "    negative =  traversal_data(neg_dir, image_size)\n",
    "    return positive, negative\n",
    "\n",
    "# # Given the dataset path and the image size to resize, load the training and testing sets\n",
    "dataset = r\"C:\\Users\\Xiaolong\\Desktop\\peopleDet\\DataSet\"\n",
    "image_size=(32, 96)\n",
    "train = os.path.join(dataset, 'train')\n",
    "test = os.path.join(dataset, 'test')\n",
    "\n",
    "# Print the number of loaded samples for both training and testing sets\n",
    "train_positive, train_negative = load_dataset(os.path.join(train, 'pos'), os.path.join(train, 'neg'), image_size)\n",
    "test_positive, test_negative = load_dataset(os.path.join(test, 'pos'), os.path.join(test, 'neg'), image_size)\n",
    "print(f\"Loaded {len(train_positive)} positive samples and {len(train_negative)} negative samples.\")\n",
    "print(f\"Loaded {len(test_positive)} positive samples and {len(test_negative)} negative samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data preprocessing: Extract HOG features and generate corresponding labels, stored in X and y respectively, using default HOG parameters.\n",
    "Train a non-linear SVM model, select the best model parameters through cross-validation, and save the corresponding model.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from itertools import chain\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# positive: samples containing human bodies; negative: samples without human bodies\n",
    "# Extract HOG features and generate corresponding labels, stored in X and y respectively, using default HOG parameters\n",
    "X_train = np.array([hog(im) for im in chain(train_positive, train_negative)])\n",
    "y_train = np.hstack([np.ones(len(train_positive)), np.zeros(len(train_negative))])\n",
    "X_test = np.array([hog(im) for im in chain(test_positive, test_negative)])\n",
    "y_test = np.hstack([np.ones(len(test_positive)), np.zeros(len(test_negative))])\n",
    "\n",
    "# Data standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "joblib.dump(scaler, r\"model\\scaler.pkl\")\n",
    "\n",
    "# model = LinearSVC()  # Linear\n",
    "model = SVC() # Non-linear\n",
    "\n",
    "# Select the best model parameters through cross-validation and save the model\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],  # Kernel function type\n",
    "    'gamma': ['scale', 'auto', 0.1, 0.01, 0.001]  # Kernel function parameter\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Train\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Output the best parameters and the best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "# Use the best parameters to fit the test set\n",
    "model_best_svm = grid_search.best_estimator_\n",
    "print(\"Test set score: {:.2f}\".format(model_best_svm.score(X_test, y_test)))\n",
    "\n",
    "joblib.dump(model_best_svm, r\"model\\model_best_svm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using a sliding window mechanism to traverse different sized images, \n",
    "extract HOG features from the current window, \n",
    "calculate distances using the SVM model, and filter out redundant boxes through NMS.\n",
    "\"\"\"\n",
    "import cv2\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "from skimage.feature import hog\n",
    "from imutils.object_detection import non_max_suppression\n",
    "\n",
    "# Sliding window\n",
    "def sliding_window(image, step_size, window_size):\n",
    "    for y in range(0, image.shape[0] - window_size[1] + 1, step_size):\n",
    "        for x in range(0, image.shape[1] - window_size[0] + 1, step_size):\n",
    "            yield (x, y, image[y:y + window_size[1], x:x + window_size[0]])\n",
    "\n",
    "# Pyramid\n",
    "def pyramid(image, scale=1.2, min_size=(64, 128)):\n",
    "    yield image\n",
    "    while image.shape[0] > min_size[1] and image.shape[1] > min_size[0]:\n",
    "        image = cv2.resize(image, \n",
    "                           (int(image.shape[1] / scale), int(image.shape[0] / scale)), \n",
    "                           interpolation=cv2.INTER_LINEAR)\n",
    "        yield image\n",
    "\n",
    "# Adjust the size of the box\n",
    "def adjust_detection_boxes(detections):\n",
    "    adjusted_boxes = []\n",
    "    for (x1, y1, x2, y2, scale) in detections:\n",
    "        adjusted_boxes.append((int(x1 * scale), int(y1 * scale), int(x2 * scale), int(y2 * scale)))\n",
    "    return adjusted_boxes\n",
    "\n",
    "# Sliding window detection\n",
    "def detect(image, svm, scaler, window_size, step_size, scale, min_size, dis_thre):\n",
    "    detections = []\n",
    "    for resized_image in pyramid(image, scale, min_size):\n",
    "        # Calculate the scaling factor for the current pyramid layer\n",
    "        scale = image.shape[0] / float(resized_image.shape[0])  \n",
    "        for (x, y, window) in sliding_window(resized_image, step_size, window_size):\n",
    "            if window.shape[:2] != (window_size[1], window_size[0]):\n",
    "                continue\n",
    "            # Extract HOG features, using default hog parameters\n",
    "            features = hog(window)\n",
    "            features = scaler.transform([features])\n",
    "            # SVM calculates the distance of the test sample to the decision plane\n",
    "            dis = svm.decision_function(features)\n",
    "            if dis > dis_thre:\n",
    "                detections.append((x, y, x + window_size[0], y + window_size[1], scale))\n",
    "    detections = adjust_detection_boxes(detections)\n",
    "    return detections\n",
    "\n",
    "# Filter out redundant boxes through nms\n",
    "def apply_nms(detections, overlap_thresh=0.1):\n",
    "    rects = np.array([[x, y, x_end, y_end]for (x, y, x_end, y_end) in detections])\n",
    "    return non_max_suppression(rects, probs=None, overlapThresh=overlap_thresh)\n",
    "\n",
    "# Test process\n",
    "imgp = r\"test_imgs\\1.jpeg\"\n",
    "image_bgr = cv2.imread(imgp)\n",
    "image_gray = cv2.cvtColor(image_bgr.copy(), cv2.COLOR_BGR2GRAY)\n",
    "Scaler = joblib.load(r\"model\\scaler.pkl\")\n",
    "SVM_model= joblib.load(r\"model\\model_best_svm.pkl\")\n",
    "\n",
    "window_size = (32, 96)  # Sliding window size, must be consistent with the size of the feature extracted during training\n",
    "step_size = 8           # Sliding window step size\n",
    "min_size = (64, 128)    # Final size of the image pyramid\n",
    "scale = 1.2             # Scaling factor for the image pyramid\n",
    "dis_thre = 1.0          # Threshold: distance of the sample point to the decision plane\n",
    "\n",
    "detections = detect(image_gray, SVM_model, Scaler, window_size, step_size, scale, min_size, dis_thre)\n",
    "final_detections = apply_nms(detections)\n",
    "\n",
    "for (x, y, x_end, y_end) in final_detections:\n",
    "    cv2.rectangle(image_bgr, (x, y), (x_end, y_end), (0, 255, 0), 2)\n",
    "plt.imshow(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
